\documentclass[titlepage,11pt,twoside]{article}


\usepackage[myheadings]{fullpage}
\usepackage{pmetrika}
%\usepackage{pmbib}


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{upgreek}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{arydshln}
\usepackage{cite}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

% to make the words are in a line on column
\usepackage{ragged2e}
\justifying


%\usepackage{submit}

\newcommand{\bfU}{\mbox{\boldmath$\mathsf{U}$}}
\newcommand{\bfu}{\mbox{\boldmath$\mathsf{u}$}}

\newcommand{\Eta}{\mbox{$\mathsf{H}$}}
\newcommand{\subEta}{\mbox{\scriptsize $\mathsf{H}$}}
\newcommand{\uni}{\mbox{\scriptsize $\mathsf{UNI}$}}

%\raggedbottom
\flushbottom


%\firstpage{1}
%\setcounter{lastpage}{999}
\setcounter{secnumdepth}{3}

\begin{document}


\linespacing{1}

\title{Solutions of Orthogonal Procrustes Problems under Partially Known columns}

\author{Jun Xu and David Zhang }

\markboth{Psychometrika}{ }

\affil{Department of Computing, The Hong Kong Polytechnic University}


\linespacing{1}

%\RepeatTitle{Psychometrics: From Practice to Theory and Back}

\begin{center}\vskip3pt


\vspace{32pt}

Abstract\vskip3pt

\end{center}


\begin{abstract}
The orthogonal Procrustes problem aims to find an orthogonal matrix $\mathbf{T}$ which transforms one given matrix $\mathbf{A}$ into another one $\mathbf{B}$ by minimizing the residual difference under Frobenius norm of matrix, i.e., $\min\|\mathbf{B}-\mathbf{T}\mathbf{A}\|_{F}^{2}$. This problem can be applied in factor analysis, dictionary learning, camera calibration, and robotics, \emph{etc}. In many real applications, some certain factors may have been partially known previously. The prior information could be formulated as partially known columns in $\mathbf{T}$ and would influence the solution of the whole orthogonal transformation matrix, and hence makes the original orthogonal Procrustes problem more difficult. In this paper, we studied the solution of orthogonal Procrustes problem under partially known columns, which includes the original orthogonal Procrustes problem as a special case.
\begin{keywords}
Orthogonal Procrustes problem, partially known columns
\end{keywords}
\end{abstract}

\section{Introduction}
Let $\mathbf{A},\mathbf{B}\in\mathbb{R}^{n\times m}$ be two given matrices of observed data, we consider the problem of transforming the matrix $\mathbf{A}$ into the matrix $\mathbf{B}$ by an orthogonal matrix $\mathbf{T}\in\mathbb{R}^{n\times n}$ so that the residual difference between $\mathbf{B}$ and $\mathbf{T}\mathbf{A}$ is minimized under the least square sense. This is the classical orthogonal Procrustes problem \cite{procrustesprogram} which can be formally stated as follows:
\begin{equation}\label{e1}
\min_{\mathbf{T}}\|\mathbf{B}-\mathbf{T}\mathbf{A}\|_{F}^{2}
\quad
\text{s.t.}
\quad
\mathbf{T}^{\top}\mathbf{T} = \mathbf{I}_{n\times n},
\end{equation}
where ``$\top$'' stands for the transpose operation. As Sch{\"o}nemann pointed out in \cite{schonemann1966generalized}, the solution of the orthogonal transformation matrix $\mathbf{T}$ for problem (\ref{e1}) is
$\mathbf{\hat{T}}=\mathbf{U}\mathbf{V}^{\top}$, where $\mathbf{U}\in\mathbb{R}^{n\times n}$ and $\mathbf{V}\in\mathbb{R}^{n\times n}$ are obtained by performing singular value decomposition (SVD) on $\mathbf{B}\mathbf{A}^{\top}$ via $\mathbf{B}\mathbf{A}^{\top}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$.

Many researchers have studied this problem in different perspectives. Green \cite{green1952orthogonal} derived the solution of problem (\ref{e1}) when the matrices $\mathbf{A}$ and $\mathbf{B}$ are of full column rank. Sch{\"o}nemann \cite{schonemann1966generalized} generalized the solution of problem (\ref{e1}) to matrices $\mathbf{A}$ and $\mathbf{B}$ of arbitrary rank. Sch{\"o}nemann further studied the two-sided orthogonal Procrustes problems \cite{schonemann1968two}. Lissitz \emph{et al.} \cite{Lissitz1976} and Koschat \cite{Koschat1991} studied the weighted orthogonal Procrustes problems, in which the weights are arranged as a diagonal matrix. More details about weighted orthogonal Procrustes problems can be found in Viklands's PhD thesis \cite{viklands2006algorithms}. Higham \cite{higham1988symmetric} studied the symmetric Procrustes problem by replacing the orthogonal restriction of $\mathbf{T}^{\top}\mathbf{T}=\mathbf{I}_{n\times n}$ with the symmetric restriction of $\mathbf{T}^{\top}=\mathbf{T}$. Later, Watson \emph{et al.} studied the solutions of orthogonal Procrustes problems under general matrix norms such as orthogonally invariant norms \cite{Watson1994} and the commonly studied $\ell_{1}$ norm \cite{trendafilov2004l1}. The Procrustes problems under orthogonally invariant norms usually could not be solved in closed-form. Instead, iterative manners are proposed which involved techniques such as projected gradient methods \cite{chu1990projected} and the famous Newton's method. In \cite{trendafilov2004l1}, the weighted orthogonal Procrustes problem is also solved by projected gradient methods \cite{chu1990projected}. Berge \cite{Berge2006} further discussed the rigid rotation problems which require the determinant of the orthogonal transformation matrix $\mathbf{T}$ to be 1 (i.e., eliminating the possibility of reflection or flip).

The classical orthogonal Procrustes problem \cite{procrustesprogram} has wide applications in different research fields, such as factor analysis \cite{green1952orthogonal}, rigid rotation \cite{Berge2006} in robotics, dimensional reduction \cite{zou2006sparse} in machine learning, camera calibration \cite{zhang2000flexible} and point matching \cite{pointpatterns} in computer vision, dictionary learning \cite{aharon2006img} in signal processing, \emph{etc}. The extended two-sided orthogonal Procrustes problem has been applied into matrix perturbation problems \cite{schonemann1968two}.

In this paper, we study the solution of the orthogonal Procrustes problem when partial columns of the orthogonal transformation matrix $\mathbf{T}$ are known. This is a problem emerged in many practical applications. For examples, in camera calibration, some certain axis is usually fixed for modeling the calibration process \cite{zhang2000flexible}, \emph{etc}. In the point matching problem \cite{pointpatterns}, if a part of the points have already been matched, the matched data could be used to guide the matching of the remaining points. In signal processing problems, dictionary learning \cite{aharon2006img} is widely used for image recovery. Some dictionary items can be learned from external clean images beforehand, and can be employed to guide the learning of the dictioanry items for the internal degraded image. The overall learned dictionary would integrate the advantages of both sides (i.e., external and internal) as well as be more adaptive to the degraded image being processed. We give a sufficiency and necessary conditions for the solution of this new emerged problem. Specifically, we presented a new proof on the sufficiency condition of the solution by using the generalized Kristof's Theorem \cite{TenBerge1983}, which is different from the proof in \cite{schonemann1966generalized}. We also discuss the number of solutions when the matrices $\mathbf{A}$ and $\mathbf{B}$ are in nonsingular or singular conditions.

\section{Definition of the Problem and Solution}
Assume $\mathbf{A},\mathbf{B}\in \mathbb{R}^{n\times m}$ be two given data matrices. Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be the partially known columns. Eventhough the partially known columns may not be stacked together, we can stack them together into a matrix $\mathbf{X}$ by resorting the matrix columns. Define $\mathbf{P}\in\mathbb{R}^{n\times q}$ ($p+q=n$) as the unknown orthogonal matrix with the remaining columns $\mathbf{P}$. From Section 1, we can see that $[\mathbf{X}\ \mathbf{P}]=\mathbf{T}\in\mathbb{R}^{n\times n}$ is the whole orthonormal matrix which transforms the matrix $\mathbf{A}$ to best fit the matrix $\mathbf{B}$ under the least square sense. Instead of obtaining $\mathbf{T}$ in classical orthogonal Procrustes problem, we need to obtain partial columns of $\mathbf{T}$ under the restriction that partial columns are already given. This problem is more difficult than the original orthogonal Procrustes problem. 

For simplicity, we assume $n\ge m$ and the other cases could be analyzed in a similar way. We formulate the orthogonal Procrustes problem with partially known columns as follows:
\begin{equation}\label{e2}
\mathbf{\hat{P}}=\arg\min_{\mathbf{D}}\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}]\mathbf{A}\|_{F}^{2}
\quad
\text{s.t.}
\quad
\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}.
\end{equation} 
Note that the original orthogonal Procrustes problem is a special case of the problem (\ref{e2}): when there is no prior known column at all, i.e., $\mathbf{X}=\oldemptyset$, then we have $p=0,q=n$ and the studied problem is reduced to the original orthogonal Procrustes problem. 
When the partially known columns $\mathbf{X}\neq\oldemptyset$, we crop the matrix $\mathbf{A}$ into two parts: $\mathbf{A}_{X}\in\mathbb{R}^{p\times m}$ and $\mathbf{A}_{P}\in\mathbb{R}^{q\times m}$ to interact with $\mathbf{X}$ and $\mathbf{P}$, respectively. Then we have 
\begin{equation}
\begin{split}
&
\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}]\mathbf{A}\|_{F}^{2}
=\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}][\mathbf{A}_{X}^{\top}\ \mathbf{A}_{P}^{\top}]^{\top}\|_{F}^{2}
=\|\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top} - \mathbf{P}\mathbf{A}_{P}^{\top}\|_{F}^{2}.
\end{split}
\end{equation}
Here, $\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top}$ is a known data matrix and we replace it with 
$\mathbf{B}^{*}=\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top}$. In the following Results 1, we remove the notation $*$ and use $\mathbf{B}$ to replace $\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top}$ as the finally known data matrix. Besides, we remove the subscript $P$ in $\mathbf{A}_{P}$ for symbol simplicity.

\textbf{Results 1}: Let $\mathbf{A}\in \mathbb{R}^{q\times m}$, $\mathbf{B}\in \mathbb{R}^{n\times m}$ be two given data matrices, given partially known prior of $\mathbf{X}^{\top}\mathbf{X}=\mathbf{I}_{p\times p}$. Then the sufficiency and necessary conditions of
\begin{equation}
\mathbf{\hat{P}}=\arg\min_{\mathbf{P}}\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
\quad
s.t.
\quad
\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q} 
\end{equation}
is $\mathbf{\hat{P}} = \mathbf{U}\mathbf{V}^{\top}$, where $\mathbf{U}\in \mathbb{R}^{n\times q}$ and $\mathbf{V}\in \mathbb{R}^{q\times q}$ are the orthogonal matrices obtained by performing economy (a.k.a. reduced) SVD:
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top} = \mathbf{U}\mathbf{\mathbf{\Sigma}}\mathbf{V}^{\top}
\end{equation}

\emph{Proof}:
Since $\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}$, we have
\begin{equation}
\begin{split}
\mathbf{\hat{P}}
&
=\arg\min_{\mathbf{P}}\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
=\arg\min_{\mathbf{P}}\|\mathbf{B}\|_{F}^{2}+\|\mathbf{P}\mathbf{A}\|_{F}^{2}-2\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})
=\arg\max_{\mathbf{P}}\text{Tr}(\mathbf{A}\mathbf{B}^{\top}\mathbf{P}).
\end{split}
\end{equation}
We can use Lagrange multiplier method and define the Lagrange function as:
\begin{equation}
\mathcal{L}
=
\text{Tr}(\mathbf{A}\mathbf{B}^{\top}\mathbf{P})
-
\text{Tr}(\Gamma_{1}(\mathbf{P}^{\top}\mathbf{P} - \mathbf{I}_{q\times q}))
-
\text{Tr}(\Gamma_{2}(\mathbf{P}^{\top}\mathbf{X}))
,
\end{equation}
where $\Gamma$ is the Lagrange multiplier. Take the derivative of $\mathcal{L}$ with respect to $\mathbf{P}$ and set it to 0, we can get
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{P}} 
=
\mathbf{B}\mathbf{A}^{\top}
-
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})
-
\mathbf{X}\Gamma_{2}^{\top}
=
0.
\end{equation}
Since $\mathbf{P}^{\top}\mathbf{P}=\mathbf{I}_{q\times q}$ and $\mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}$, by left multiplying the Eq. (7) by $\mathbf{X}^{\top}$, we have 
\begin{equation}
\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
\Gamma_{2}^{\top}.
\end{equation}
Put the results back to Eq. (7), we have 
\begin{equation}
\mathbf{B}\mathbf{A}^{\top}
-
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})
-
\mathbf{X}\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
0.
\end{equation}
Or equivalently,
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top}).
\end{equation}
Right multiplying Eq. (10) by $\mathbf{P}^{\top}$, we have
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}
=
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})\mathbf{P}^{\top}
.
\end{equation}
This shows that $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}$ is a symmetric matrix of order $n\times n$. Then we perform economy (or reduced) singular value decomposition (SVD) on $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}$ and get 
$(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$.
Since $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}$ is symmetric, we have
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}
=
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top}
=
\mathbf{P}\mathbf{V}\mathbf{\Sigma}\mathbf{U}^{\top}
\end{equation}
and hence we have $\mathbf{U}=\mathbf{P}\mathbf{V}$ and equivalently $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$. Note that we can also employ the property of symmetric matrix that every symmetric matrix could be diagonalized to obtain this result. The necessary condition is proofed. 

Now we proof the sufficiency condition. If $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$, then $\mathbf{\hat{P}}$ satisfies that $\mathbf{\hat{P}}^{\top}\mathbf{\hat{P}}=\mathbf{I}_{q\times q}$ and $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{0}_{p\times q}$. The first is obvious and now we consider the second one. From the Eq. (4), since $\mathbf{X}^{\top}\mathbf{X}=\mathbf{I}_{p\times p}$, we have  
\begin{equation}
\mathbf{X}^{\top}(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}=\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}-\mathbf{X}^{\top}\mathbf{X}\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{0}_{p\times n}
.
\end{equation}
It means that $\mathbf{X}^{\top}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}=\mathbf{0}_{p\times p}$ and hence $\mathbf{X}^{\top}\mathbf{U}=\mathbf{0}_{p\times p}$. Then $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{X}^{\top}\mathbf{U}\mathbf{V}^{\top}=\mathbf{0}_{p\times q}$.

Besides, since
\begin{equation}
\begin{split}
\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
=\|\mathbf{B}\|_{F}^{2}+\|\mathbf{P}\mathbf{A}\|_{F}^{2}-2\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A}),
\end{split}
\end{equation}
Until now, if we want to proof that $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$ is the solution of problem (3), $\text{Tr}(\mathbf{B}^{\top}\mathbf{\hat{P}}\mathbf{A})$ has to be a maximum if $\|\mathbf{B}-\mathbf{\hat{P}}\mathbf{A}\|_{F}^{2}$ is to be a minimum, as long as $\mathbf{P}$ satisfying the conditions in Eq. (3).
Note that by cyclic perturbation which retains the trace unchanged and due to $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{0}_{p\times q}$, we have 
\begin{equation}
\begin{split}
\text{Tr}(\mathbf{B}^{\top}\mathbf{\hat{P}}\mathbf{A})
&
=
\text{Tr}(\mathbf{B}\mathbf{A}^{\top}\mathbf{\hat{P}}^{\top})
\\
&
=
\text{Tr}((\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{\hat{P}}^{\top})
\\
&
=
\text{Tr}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{V}\mathbf{U}^{\top})
\\
&
=
\text{Tr}(\mathbf{\Sigma}).
\end{split}
\end{equation}
Now we need to proof that $\text{Tr}(\mathbf{\Sigma})\ge\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})$ for every $\mathbf{P}$ satisfying that $\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}$. 
Since $\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})
=
\text{Tr}((\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top})
=
\text{Tr}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top})
=
\text{Tr}(\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top}\mathbf{U})
\le
\text{Tr}(\mathbf{\Sigma})
.
$
The last inequality can be obtained by using a generalization version \cite{TenBerge1983} of the Kristof's Theorem \cite{Kristof1970515}. The equality is obtained at 
$\mathbf{V}^{\top}\mathbf{P}^{\top}\mathbf{U}=\mathbf{I}_{q\times q}$, i.e., $\mathbf{P}=\mathbf{U}\mathbf{V}^{\top}=\mathbf{\hat{P}}$. This completes the proof.$\hfill\blacksquare$ 

Note that if the partially known prior were not present, the solution is clearly the solution of the original orthogonal Procrustes problem, i.e., $\mathbf{\hat{P}} = \mathbf{U}\mathbf{V}^{\top}$, where $\mathbf{U}$ and $\mathbf{V}$ are the orthogonal matrices obtained by performing economy (a.k.a. reduced) SVD:
$\mathbf{B}\mathbf{A}^{\top} = \mathbf{U}\mathbf{\mathbf{\Sigma}}\mathbf{V}^{\top}$. 
The difference between the solutions of the original orthogonal Procrustes problem and its partially known prior version quantify the effect on the residual of requiring $\mathbf{P}$ to be orthogonal to the external prior $\mathbf{P}^{\top}\mathbf{X}$.


\section{Uniqueness of Solution $\mathbf{\hat{P}}$}

Now we discuss the uniqueness of the solution $\mathbf{\hat{P}}$. Our discussion is according to the rankness of the $\mathbf{\Sigma}$ generated in the SVD. We first give a lemma describing the rank of $\mathbf{X}\mathbf{X}^{\top}$ where $\mathbf{X}\in\mathbb{R}^{n\times p}$ is the partially known orthogonal matrix. 

\emph{Lemma 1}: Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be orthogonal matrix with $\mathbf{X}^{\top}\mathbf{X}=\mathbf{I}_{p\times p}$, then $\text{rank}(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\ge n-p$.

\emph{Proof}: We firstly proof that $\text{rank}(\mathbf{X}\mathbf{X}^{\top})=p$. The upper bound of the $\text{rank}(\mathbf{X}\mathbf{X}^{\top})$ can be determined by $\text{rank}(\mathbf{X}\mathbf{X}^{\top})\le\min\{\text{rank}(\mathbf{X}),\text{rank}(\mathbf{X}^{\top})\}=p$. The lower bound of the $\text{rank}(\mathbf{X}\mathbf{X}^{\top})$ can be determined by Sylvester's inequality as $\text{rank}(\mathbf{X}\mathbf{X}^{\top})\ge\text{rank}(\mathbf{X})+\text{rank}(\mathbf{X}^{\top})-p=2p-p=p$. Hence, we have $\text{rank}(\mathbf{X}\mathbf{X}^{\top})=p$. Then, $\text{rank}(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\ge\text{rank}(\mathbf{I}_{n\times n})-\text{rank}(\mathbf{X}\mathbf{X}^{\top})\ge n-p$. 
$\hfill\blacksquare$ 

The rank of $\mathbf{\Sigma}$ largely depends on the rank of $\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top}$, $\mathbf{B}$ and $\mathbf{A}$. Note that the rank of $\mathbf{B}$ and $\mathbf{A}$ are not larger than $m$ and $q$, respectively. The rank of $\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top}$ is at least $n-q=p$ (since rank($\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top}$) $\ge$ $\text{rank}(\mathbf{I}_{n\times n}) - \text{rank}(\mathbf{X}\mathbf{X}^{\top})$ $\ge$ $n-p=q$). From above observations, we can see that the rank of $\mathbf{\Sigma}$ can be equal to or less than $q$. 

\textbf{Results 2}: If $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}$ is nonsingular, then $\text{rank}(\mathbf{\Sigma})=q$, and $\mathbf{\Sigma}$ may have distinct or multiple non-zero singular values. In the SVD of $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$, the singular vectors in $\mathbf{U}$ and $\mathbf{V}$
can be determined up to orientation. Hence, we can reformulate the SVD as 
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{U}^{*}\mathbf{K}_{u}\mathbf{\Sigma}\mathbf{K}_{v}(\mathbf{V}^{*})^{\top},
\end{equation}
where $\mathbf{U}^{*}\in \mathbb{R}^{n\times q}$ and $\mathbf{V}^{*}\in \mathbb{R}^{q\times q}$ are arbitrarily orientated singular vectors of $\mathbf{U}$ and $\mathbf{V}$, respectively. $\mathbf{\Sigma}\in \mathbb{R}^{q\times q}$ are diagonal matrix with singular values are arranged in weak descending order along the diagonal, i.e., $\mathbf{\Sigma}_{11}\ge\mathbf{\Sigma}_{22}\ge...\ge\mathbf{\Sigma}_{qq}\ge0$. The $\mathbf{K}_{u}$ and $\mathbf{K}_{v}$ are diagonal matrices with $+1$ or $-1$ as diagonal elements in arbitrary distribution. If we fix $\mathbf{K}_{u}$, then $\mathbf{K}_{v}$ is uniquely determined to meet the requirement that the diagonal elements of $\mathbf{\Sigma}$ should be nonnegative. And the orientations of the singular vectors of $\mathbf{U}^{*}$ is fixed, then the $\mathbf{U}=\mathbf{U}^{*}\mathbf{K}_{u}$ is determined, so does the orientations of the singular vectors of $\mathbf{V}^{*}$ and $\mathbf{V}^{\top}=\mathbf{K}_{v}(\mathbf{V}^{*})^{\top}$. In this case, the solution of $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}=\mathbf{U}^{*}\mathbf{K}_{u}\mathbf{K}_{v}(\mathbf{V}^{*})^{\top}$ is unique. The case that the $\mathbf{\Sigma}$ have multiple singular values also has unique solution of $\mathbf{\hat{P}}$, which can be discussed in a similar way. 

If $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}$ is singular, then $0\le\text{rank}(\mathbf{\Sigma})=r< q$, and there is $q-r$ (at least one) zero singular values. The previous discussion on non-degenerative case still can be applied to the singular vectors related to the nonzero singular values, and this part is still unique. However, the singular vectors related to the zero singular values could be in arbitrary orientations as long as they satisfy the orthogonal conditions that $\mathbf{U}^{\top}\mathbf{U}=\mathbf{V}^{\top}\mathbf{V}=\mathbf{V}\mathbf{V}^{\top}=\mathbf{I}_{q\times q}$. Note that $\mathbf{U}\in \mathbb{R}^{n\times q}$, so $\mathbf{U}\mathbf{U}^{\top}$ no longer equals to the identity matrix of order $n$. From Eq. (12), we can get 
\begin{equation}
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top}
=
\mathbf{P}\mathbf{V}\mathbf{\Sigma}\mathbf{U}^{\top}
\end{equation}
Right multiplying each side by $\mathbf{P}\mathbf{V}$ and then left multiplying each side by $\mathbf{U}^{\top}$, we can get  
\begin{equation}
\mathbf{\Sigma}
=
\mathbf{U}^{\top}\mathbf{P}\mathbf{V}\mathbf{\Sigma}\mathbf{U}^{\top}\mathbf{P}\mathbf{V}
\end{equation}
Hence, we can define a diagonal matrix $\mathbf{D}=\mathbf{U}^{\top}\mathbf{P}\mathbf{V}\in\mathbb{R}^{q\times q}$, the diagonal elements of which are 
\begin{displaymath}
\mathbf{D}_{ii}= \left\{ \begin{array}{ll}
1 & \textrm{if $1\le i\le r$};\\
\pm 1 & \textrm{if $r< i \le q$}.\\
\end{array} \right.
\end{displaymath}
Thus, we obtain that $\mathbf{P}=\mathbf{U}\mathbf{D}\mathbf{V}^{\top}$, where $\mathbf{D}$ is defined above. Hence, once we get the solution of $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$ in problem (3), the final solution for $\mathbf{P}$ when $\text{rank}(\mathbf{\Sigma})<q$ is not unique since the matrix $\mathbf{D}$ is not uniquely determined. In fact, since the number of $\mathbf{D}$ with different diagonal combinations is $2^{q-r}$, the number of solutions for $\mathbf{P}$ is $2^{q-r}$ given fixed $\mathbf{U}$ and $\mathbf{V}$.

%Since the solution of $\mathbf{\hat{P}}$ is not unique when $\text{rank}(\mathbf{\Sigma})<q$, we define the set of solutions for in a formal manner and discuss its properties. The solution set can be defined as:
%\begin{equation}
%\mathcal{S}=\{\mathbf{S}\in\mathbb{R}^{n\times q}: \mathbf{S}^{\top}\mathbf{S}=\mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{S}=\mathbf{0}_{p\times q}, \|\mathbf{B}-\mathbf{S}\mathbf{A}\|_{F}^{2}=\min_{\mathbf{P}}\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}\}
%\end{equation}

%\section{Sensitivity of $\mathbf{\hat{P}}$ to Data Perturbations}

%In this section, we examine the sensitivity of the solution to perturbation in the data. To measure this sensitivity, we give the relative residuals and the Fro-norm condition numbers of the solutions. The condition number of the matrix $\mathbf{A}$ is defined as $k_{F}(\mathbf{A})=\frac{\sigma_{1}}{\sigma_{r}}$, where $r=\text{rank}(\mathbf{A})$. 


%\begin{figure}[h]
%\centerline{\includegraphics[width=254pt]{/figure07.eps}}
%\caption{North Carolina End-of-Grade Math Skills Test Subscores.}
%\end{figure}



\section{Concluding Remarks}
In this paper, we studied the classical orthogonal Procrustes problem and gave the solution of this problem under partially known guidance, which included the original orthogonal Procrustes problem as a special case (i.e., when there is no guidance at all). Since the orthogonal Procrustes problem had been generalized to two-sided version \cite{schonemann1968two} and weighted version \cite{Lissitz1976,Koschat1991}, we are highly motivated to study these generalized problems under partially known guidance.

\linespacing{0.5}
\bibliographystyle{unsrt}%{ieee}%{unsrt}
\bibliography{egbib}


%\vfill\eject
\end{document}
