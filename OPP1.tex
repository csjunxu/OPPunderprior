\documentclass[titlepage,11pt,twoside]{article}


\usepackage[myheadings]{fullpage}
\usepackage{pmetrika}
%\usepackage{pmbib}


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{upgreek}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{arydshln}
\usepackage{cite}

% to make the words are in a line on column
\usepackage{ragged2e}
\justifying


%\usepackage{submit}

\newcommand{\bfU}{\mbox{\boldmath$\mathsf{U}$}}
\newcommand{\bfu}{\mbox{\boldmath$\mathsf{u}$}}

\newcommand{\Eta}{\mbox{$\mathsf{H}$}}
\newcommand{\subEta}{\mbox{\scriptsize $\mathsf{H}$}}
\newcommand{\uni}{\mbox{\scriptsize $\mathsf{UNI}$}}

%\raggedbottom
\flushbottom


%\firstpage{1}
%\setcounter{lastpage}{999}
\setcounter{secnumdepth}{3}

\begin{document}


\linespacing{1}

\title{Solutions of Orthogonal Procrustes Problems under Partially Known  Prior}

\author{Jun Xu }

\markboth{Psychometrika}{ }

\affil{Department of Computing, The Hong Kong Polytechnic University}


\linespacing{1}

%\RepeatTitle{Psychometrics: From Practice to Theory and Back}

\begin{center}\vskip3pt


\vspace{32pt}

Abstract\vskip3pt

\end{center}


\begin{abstract}
The orthogonal Procrustes problem aims to find an orthogonal matrix which transforms one given matrix into another by minimizing their Frobenius matrix norm. This problem can be applied in applications such as permutation theory, machine learning, and camera calibration, \emph{etc}. In real cases, the permutation matrix may have been partially known, the dictionaries can be partially learned from external data, and the calibration of camera should be done under some fixed priors. This prior information makes the original orthogonal Procrustes problem more difficult. In this paper, we consider the solution of this problem under partially known priors, which includes the original orthogonal Procrustes problem as a special case with no such prior.
\begin{keywords}
orthogonal Procrustes problem, pratially known priors
\end{keywords}
\end{abstract}

\section{Introduction}
The classical orthogonal Procrustes problem has been applied in psychometrics, multidimensional scaling, factor analysis, machine learning, computer vision, optical imaging, and robotics.

\section{Definition of the Probelm and Solution}
Let $\mathbf{A},\mathbf{B}\in \mathbb{R}^{n\times m}$ be two given data matrices. Define $\mathbf{X}\in\mathbb{R}^{n\times p}$ and $\mathbf{P}\in\mathbb{R}^{n\times q}$ where $p+q=n$. For simplicity, we assume $n\ge m$ and the other cases can be analyzed in a similar way. $\mathbf{X}$ is the partially known prior which could be used to guide the solutions of $\mathbf{P}$. We formulate the orthogonal Procrustes problem with partially known priors as:
\begin{equation}
\mathbf{\hat{P}}=\arg\min_{\mathbf{D}}\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}]\mathbf{A}\|_{F}^{2}
\quad
s.t.
\quad
\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}.
\end{equation} 
In fact, as have been proofed, if the matrix $\mathbf{\mathbf{B}\mathbf{A}^{\top}}$ has no zero singular value, then the solution of  
$\mathbf{\hat{P}} = \mathbf{U}\mathbf{V}^{\top}$ is unique and we do not need any preceding results.

We crop the matrix $\mathbf{A}$ into two parts: $\mathbf{A}_{X}\in\mathbb{R}^{p\times m}$ and $\mathbf{A}_{P}\in\mathbb{R}^{q\times m}$ to interact with $\mathbf{X}$ and $\mathbf{P}$, respectively. Then we have 
\begin{equation}
\begin{split}
&
\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}]\mathbf{A}\|_{F}^{2}
=\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}][\mathbf{A}_{X}^{\top}\ \mathbf{A}_{P}^{\top}]^{\top}\|_{F}^{2}
=\|\mathbf{B}-[\mathbf{X}\ \mathbf{P}][\mathbf{A}_{X}^{\top}\ \mathbf{A}_{P}^{\top}]^{\top}\|_{F}^{2}
\\
&
=\|\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top} - \mathbf{P}\mathbf{A}_{P}^{\top}]\|_{F}^{2}
=\|\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top} - \mathbf{P}\mathbf{A}_{P}^{\top}]\|_{F}^{2}
\end{split}
\end{equation}
The $\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top}$ is a known data matrix and we replace it with 
$\mathbf{B}^{*}=\mathbf{B}-\mathbf{X}\mathbf{A}_{X}^{\top}$. In the following Results 1, we remove the notation $*$ and use $\mathbf{B}$ as the finally known data matrix.

\textbf{Results 1}: Let $\mathbf{A}\in \mathbb{R}^{q\times m}$, $\mathbf{B}\in \mathbb{R}^{n\times m}$ be two given data matrices, given partially known prior of $\mathbf{X}^{\top}\mathbf{X}=\mathbf{I}_{p\times p}$. Then the sufficiency and necessary conditions of
\begin{equation}
\mathbf{\hat{P}}=\arg\min_{\mathbf{P}}\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
\quad
s.t.
\quad
\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q} 
\end{equation}
is $\mathbf{\hat{P}} = \mathbf{U}\mathbf{V}^{\top}$, where $\mathbf{U}\in \mathbb{R}^{n\times q}$ and $\mathbf{V}\in \mathbb{R}^{q\times q}$ are the orthogonal matrices obtained by perfroming economy (a.k.a. reduced) SVD:
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top} = \mathbf{U}\mathbf{\mathbf{\Sigma}}\mathbf{V}^{\top}
\end{equation}

\emph{Proof}:
Since $\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}$, we have
\begin{equation}
\begin{split}
\mathbf{\hat{P}}
&
=\arg\min_{\mathbf{P}}\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
=\arg\min_{\mathbf{P}}\|\mathbf{B}\|_{F}^{2}+\|\mathbf{P}\mathbf{A}\|_{F}^{2}-2\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})
=\arg\max_{\mathbf{P}}\text{Tr}(\mathbf{A}\mathbf{B}^{\top}\mathbf{P}).
\end{split}
\end{equation}
We can use Lagrange multiplier method and define the Lagrange function as:
\begin{equation}
\mathcal{L}
=
\text{Tr}(\mathbf{A}\mathbf{B}^{\top}\mathbf{P})
-
\text{Tr}(\Gamma_{1}(\mathbf{P}^{\top}\mathbf{P} - \mathbf{I}_{q\times q}))
-
\text{Tr}(\Gamma_{2}(\mathbf{P}^{\top}\mathbf{X}))
,
\end{equation}
where $\Gamma$ is the Lagrange multiplier. Take the derivative of $\mathcal{L}$ with respect to $\mathbf{P}$ and set it to 0, we can get
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{P}} 
=
\mathbf{B}\mathbf{A}^{\top}
-
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})
-
\mathbf{X}\Gamma_{2}^{\top}
=
0.
\end{equation}
Since $\mathbf{P}^{\top}\mathbf{P}=\mathbf{I}_{q\times q}$ and $\mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}$, by left multiplying the Equ. (7) by $\mathbf{X}^{\top}$, we have 
\begin{equation}
\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
\Gamma_{2}^{\top}.
\end{equation}
Put the results back to Equ. (7), we have 
\begin{equation}
\mathbf{B}\mathbf{A}^{\top}
-
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})
-
\mathbf{X}\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
0.
\end{equation}
Or equivaliently, 
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top}).
\end{equation}
Right multiplying Equ. (10) by $\mathbf{P}^{\top}$, we have
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}
=
\mathbf{P}(\Gamma_{1}+\Gamma_{1}^{\top})\mathbf{P}^{\top}
.
\end{equation}
This shows that $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}$ is a symmertric matrix of order $n\times n$. Then we perfrom economy (or reduced) singular value decomposition (SVD) on $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}$ and get 
$(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$.
Since $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}$ is symmertric, we have
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top}
=
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top}
=
\mathbf{P}\mathbf{V}\mathbf{\Sigma}\mathbf{U}^{\top}
\end{equation}
and hence we have $\mathbf{U}=\mathbf{P}\mathbf{V}$ and equivalently $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$. Note that we can also employ the property of symmertric matrix that every symmertric matrix can be diagonalized to obtain this results. The necessary condition is proofed. 

Now we proof the sufficiency condition. If $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$, then $\mathbf{\hat{P}}$ satisfies that $\mathbf{\hat{P}}^{\top}\mathbf{\hat{P}}=\mathbf{I}_{q\times q}$ and $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{0}_{p\times q}$. The first is obvious and now we consider the second one. From the Equ. (4), since $\mathbf{X}^{\top}\mathbf{X}=\mathbf{I}_{p\times p}$, we have  
\begin{equation}
\mathbf{X}^{\top}(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}=\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}-\mathbf{X}^{\top}\mathbf{X}\mathbf{X}^{\top}\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{0}_{p\times n}
.
\end{equation}
It means that $\mathbf{X}^{\top}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}=\mathbf{0}_{p\times p}$ and hence $\mathbf{X}^{\top}\mathbf{U}=\mathbf{0}_{p\times p}$. Then $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{X}^{\top}\mathbf{U}\mathbf{V}^{\top}=\mathbf{0}_{p\times q}$.

Besides, since
\begin{equation}
\begin{split}
\|\mathbf{B}-\mathbf{P}\mathbf{A}\|_{F}^{2}
=\|\mathbf{B}\|_{F}^{2}+\|\mathbf{P}\mathbf{A}\|_{F}^{2}-2\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A}),
\end{split}
\end{equation}
Until now, if we want to proof that $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}$ is the solution of problem (3), $\text{Tr}(\mathbf{B}^{\top}\mathbf{\hat{P}}\mathbf{A})$ has to be a maximum if $\|\mathbf{B}-\mathbf{\hat{P}}\mathbf{A}\|_{F}^{2}$ is to be a minimum, over all $\mathbf{P}$ satisfying the subject condition in Equ. (3).
Note that by cyclic perturbation which retains the trace unchanged and due to $\mathbf{X}^{\top}\mathbf{\hat{P}}=\mathbf{0}_{p\times q}$, we have 
\begin{equation}
\begin{split}
\text{Tr}(\mathbf{B}^{\top}\mathbf{\hat{P}}\mathbf{A})
&
=
\text{Tr}(\mathbf{B}\mathbf{A}^{\top}\mathbf{\hat{P}}^{\top})
\\
&
=
\text{Tr}((\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{\hat{P}}^{\top})
\\
&
=
\text{Tr}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{V}\mathbf{U}^{\top})
\\
&
=
\text{Tr}(\mathbf{\Sigma}).
\end{split}
\end{equation}
Now we need to proof that $\text{Tr}(\mathbf{\Sigma})\ge\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})$ for every $\mathbf{P}$ satisfying that $\mathbf{P}^{\top}\mathbf{P} = \mathbf{I}_{q\times q}, \mathbf{X}^{\top}\mathbf{P} = \mathbf{0}_{p\times q}$. 
Since $\text{Tr}(\mathbf{B}^{\top}\mathbf{P}\mathbf{A})
=
\text{Tr}((\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}\mathbf{P}^{\top})
=
\text{Tr}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top})
=
\text{Tr}(\mathbf{\Sigma}\mathbf{V}^{\top}\mathbf{P}^{\top}\mathbf{U})
\le
\text{Tr}(\mathbf{\Sigma})
.
$
The last inequality can be obtained by using a generalization version \cite{TenBerge1983} of the Kristof's Theorem \cite{Kristof1970515}. The equality is obtained at 
$\mathbf{V}^{\top}\mathbf{P}^{\top}\mathbf{U}=\mathbf{I}_{q\times q}$, i.e., $\mathbf{P}=\mathbf{U}\mathbf{V}^{\top}=\mathbf{\hat{P}}$. This completes the proof.$\hfill\blacksquare$ 

\section{Uniqueness of $\mathbf{\hat{P}}$}
Now we discuss the uniqueness of the solution $\mathbf{\hat{P}}$. In the SVD of $(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$, the singular vectors in $\mathbf{U}$ and $\mathbf{V}$
can be determined up to orientation. The rank of the matrix $\mathbf{\Sigma}$ largely depends on the matrices $\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top}$, $\mathbf{B}$ and $\mathbf{A}$. The rank of $\mathbf{B}$ and $\mathbf{A}$ are not larger than $m$ and $q$, respectively. can be equal to or less than $q$. 

Non-degenerative case of rank($\mathbf{\Sigma}$): if $\mathbf{\Sigma}=q$, the 
Hence, we can reformulate the SVD as 
\begin{equation}
(\mathbf{I}_{n\times n}-\mathbf{X}\mathbf{X}^{\top})\mathbf{B}\mathbf{A}^{\top}
=
\mathbf{U}^{*}\mathbf{K}_{u}\mathbf{\Sigma}\mathbf{K}_{v}(\mathbf{V}^{*})^{\top},
\end{equation}
where $\mathbf{U}^{*}\in \mathbb{R}^{n\times q}$ and $\mathbf{V}^{*}\in \mathbb{R}^{q\times q}$ are arbitrarily orientated singular vectors of $\mathbf{U}$ and $\mathbf{V}$, respectively. $\mathbf{\Sigma}\in \mathbb{R}^{q\times q}$ are diagonal matrix with singular values are arranged in weak descending order along the diagonal, i.e., $\mathbf{\Sigma}_{11}\ge\mathbf{\Sigma}_{22}\ge...\ge\mathbf{\Sigma}_{qq}\ge0$. The $\mathbf{K}_{u}$ and $\mathbf{K}_{v}$ are diagnonal matrices with $+1$ or $-1$ as diagonal elements in arbitrary distribution. If we fix $\mathbf{K}_{u}$, then $\mathbf{K}_{v}$ is uniquely determined to meet the requirement that the diagonal elements of $\mathbf{\Sigma}$ should be nonnegative. And the orientations of the singular vectors of $\mathbf{U}^{*}$ is fixed, then the $\mathbf{U}=\mathbf{U}^{*}\mathbf{K}_{u}$ is determined, so does the orientations of the singular vectors of $\mathbf{V}^{*}$ and $\mathbf{V}^{\top}=\mathbf{K}_{v}(\mathbf{V}^{*})^{\top}$. In this case, the solution of $\mathbf{\hat{P}}=\mathbf{U}\mathbf{V}^{\top}=\mathbf{U}^{*}\mathbf{K}_{u}\mathbf{K}_{v}(\mathbf{V}^{*})^{\top}$ is unique.

For $\text{Tr}(\mathbf{A}\mathbf{B}^{\top}\mathbf{P})$

Note that if the partially known prior were not present, the solution is clearly the solution of the original orthogonal Procrustes problem, i.e., $\mathbf{\hat{P}} = \mathbf{U}\mathbf{V}^{\top}$, where $\mathbf{U}$ and $\mathbf{V}$ are the orthogonal matrices obtained by perfroming economy (a.k.a. reduced) SVD:
$\mathbf{B}\mathbf{A}^{\top} = \mathbf{U}\mathbf{\mathbf{\Sigma}}\mathbf{V}^{\top}$. 
The difference between the solutions of the original orthogonal Procrustes problem and its partially known prior version quantify the effect on the residual of requiring $\mathbf{P}$ to be orgothonal to the external prior $\mathbf{P}^{\top}\mathbf{X}$.


In this section, we examine the sensitivity of the solution to perturbation in the data. To measure this sensitivity, we give the relative residuals and the Fro-norm condition numbers of the solutions. The condition number of the matrix $\mathbf{A}$ is defined as $k_{F}(\mathbf{A})=\frac{\sigma_{1}}{\sigma_{r}}$, where $r=\text{rank}(\mathbf{A})$. 






\begin{figure}[h]
%\centerline{\includegraphics[width=254pt]{/figure07.eps}}
\caption{North Carolina End-of-Grade Math Skills Test Subscores.}
\end{figure}



\section{Concluding Remarks}



\bibliographystyle{unsrt}%{ieee}%{unsrt}
\bibliography{egbib}


%\vfill\eject
\end{document}
